"""
å¼‚å¸¸è¡Œä¸ºæ£€æµ‹
ä½¿ç”¨å­¤ç«‹æ£®æ—ç®—æ³•æ£€æµ‹å­¦ç”Ÿçš„å¼‚å¸¸å­¦ä¹ è¡Œä¸ºï¼Œå¸®åŠ©è¯†åˆ«éœ€è¦å…³æ³¨çš„å­¦ç”Ÿ
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import classification_report
import joblib
import logging
from datetime import datetime

class AnomalyDetector:
    def __init__(self, contamination=0.2):
        """
        åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹å™¨
        contamination: å¼‚å¸¸æ¯”ä¾‹ï¼Œé»˜è®¤20%ï¼ˆé€‚åˆå°æ•°æ®é›†ï¼‰
        """
        self.contamination = contamination
        self.model = IsolationForest(contamination=contamination, random_state=42, n_estimators=50)
        self.scaler = RobustScaler()
        self.is_trained = False
        self.data_size = 'unknown'
        self.feature_names = [
            'performance_variability', 'completion_anomaly', 'engagement_anomaly',
            'learning_pattern_anomaly', 'academic_deviation', 'behavior_consistency'
        ]
        self.anomaly_types = {
            'low_engagement': 'å­¦ä¹ å‚ä¸åº¦è¿‡ä½',
            'irregular_pattern': 'å­¦ä¹ æ¨¡å¼ä¸è§„å¾‹',
            'poor_performance': 'å­¦ä¹ æˆç»©å¼‚å¸¸åä½',
            'excessive_struggle': 'å­¦ä¹ å›°éš¾ç¨‹åº¦å¼‚å¸¸',
            'inconsistent_behavior': 'è¡Œä¸ºæ¨¡å¼ä¸ä¸€è‡´'
        }
        
    def prepare_features(self, users):
        """å‡†å¤‡å¼‚å¸¸æ£€æµ‹ç‰¹å¾"""
        features = []
        user_ids = []
        
        for user in users:
            try:
                # ä½œä¸šç›¸å…³ç‰¹å¾
                homework = user.homework_statistic[0] if user.homework_statistic else None
                if homework:
                    scores = [getattr(homework, f'score{i}', 0) for i in range(2, 10)]
                    valid_scores = [s for s in scores if s > 0]
                    
                    homework_avg = np.mean(valid_scores) if valid_scores else 0
                    homework_completion_rate = len(valid_scores) / len(scores)
                    
                    # ä½œä¸šä¸€è‡´æ€§ï¼ˆæ³¢åŠ¨ç¨‹åº¦ï¼‰
                    if len(valid_scores) > 2:
                        homework_consistency = 1 / (1 + np.std(valid_scores) / (np.mean(valid_scores) + 1e-6))
                    else:
                        homework_consistency = 0
                else:
                    homework_avg = 0
                    homework_completion_rate = 0
                    homework_consistency = 0
                
                # è®¨è®ºå‚ä¸ç‰¹å¾
                discussion = user.discussion_participation[0] if user.discussion_participation else None
                if discussion:
                    discussion_posts = discussion.posted_discussions or 0
                    discussion_replies = discussion.replied_discussions or 0
                    total_discussions = discussion.total_discussions or 0
                    upvotes = discussion.upvotes_received or 0
                    
                    # è·èµç‡ï¼ˆè´¨é‡æŒ‡æ ‡ï¼‰
                    total_activity = discussion_posts + discussion_replies
                    upvotes_ratio = upvotes / max(total_activity, 1) if total_activity > 0 else 0
                else:
                    discussion_posts = 0
                    discussion_replies = 0
                    upvotes = 0
                    upvotes_ratio = 0
                
                # è§†é¢‘å­¦ä¹ ç‰¹å¾
                video = user.video_watching_details[0] if user.video_watching_details else None
                if video:
                    watch_times = [getattr(video, f'watch_duration{i}', 0) or 0 for i in range(1, 8)]
                    rumination_ratios = [getattr(video, f'rumination_ratio{i}', 0) or 0 for i in range(1, 8)]
                    
                    video_watch_time = sum(watch_times)
                    valid_ratios = [r for r in rumination_ratios if r > 0]
                    video_rumination_ratio = np.mean(valid_ratios) if valid_ratios else 0
                else:
                    video_watch_time = 0
                    video_rumination_ratio = 0
                
                # å­¦ä¹ æ¨¡å¼å¾—åˆ†ï¼ˆç»¼åˆè¯„ä¼°ï¼‰
                learning_pattern_score = (
                    homework_completion_rate * 0.3 +
                    min(discussion_posts + discussion_replies, 20) / 20 * 0.3 +
                    min(video_watch_time, 500) / 500 * 0.4
                )
                
                # å­¦æœ¯è¡¨ç°
                synthesis = user.synthesis_grades[0] if user.synthesis_grades else None
                academic_performance = synthesis.comprehensive_score if synthesis else 0
                
                # æ€»ä½“å‚ä¸åº¦å¾—åˆ†
                engagement_score = (
                    homework_completion_rate * 0.4 +
                    min(discussion_posts + discussion_replies + (upvotes * 2), 30) / 30 * 0.3 +
                    min(video_watch_time, 400) / 400 * 0.3
                )
                
                features.append([
                    homework_avg, homework_completion_rate, homework_consistency,
                    discussion_posts, discussion_replies, upvotes_ratio,
                    video_watch_time, video_rumination_ratio, learning_pattern_score,
                    academic_performance, engagement_score
                ])
                user_ids.append(user.id)
                
            except Exception as e:
                logging.warning(f"å¤„ç†ç”¨æˆ· {user.id} å¼‚å¸¸æ£€æµ‹ç‰¹å¾æ—¶å‡ºé”™: {str(e)}")
                continue
        
        return np.array(features), user_ids
    
    def train_model(self, users):
        """ä¼˜åŒ–çš„å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒ"""
        try:
            features, user_ids = self.prepare_features(users)
            
            if len(features) < 3:
                logging.warning(f"å¼‚å¸¸æ£€æµ‹æ•°æ®ä¸è¶³ï¼Œå½“å‰æœ‰{len(features)}ä¸ªæœ‰æ•ˆæ ·æœ¬ï¼Œè‡³å°‘éœ€è¦3ä¸ªæ ·æœ¬")
                return False
            
            # è‡ªé€‚åº”è°ƒæ•´å‚æ•°
            if len(features) < 10:
                self.contamination = 0.3  # å°æ•°æ®é›†æé«˜å¼‚å¸¸æ¯”ä¾‹
                self.model = IsolationForest(contamination=self.contamination, random_state=42, n_estimators=50)
                self.data_size = 'small'
                logging.info(f"å°æ•°æ®é›†æ¨¡å¼ï¼šè°ƒæ•´å¼‚å¸¸æ¯”ä¾‹ä¸º{self.contamination}")
            elif len(features) < 30:
                self.contamination = 0.2
                self.model = IsolationForest(contamination=self.contamination, random_state=42, n_estimators=100)
                self.data_size = 'medium'
                logging.info(f"ä¸­å‹æ•°æ®é›†æ¨¡å¼ï¼šå¼‚å¸¸æ¯”ä¾‹{self.contamination}")
            else:
                self.contamination = 0.1
                self.model = IsolationForest(contamination=self.contamination, random_state=42)
                self.data_size = 'large'
            
            # å¤„ç†å¼‚å¸¸å€¼
            features = self._handle_outliers(features)
            
            # æ•°æ®æ ‡å‡†åŒ–
            features_scaled = self.scaler.fit_transform(features)
            
            # è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹
            self.model.fit(features_scaled)
            
            # è·å–å¼‚å¸¸å¾—åˆ†å’Œæ ‡ç­¾
            anomaly_scores = self.model.decision_function(features_scaled)
            anomaly_labels = self.model.predict(features_scaled)
            
            # åˆ†æå¼‚å¸¸æƒ…å†µ
            anomaly_count = sum(1 for label in anomaly_labels if label == -1)
            logging.info(f"å¼‚å¸¸æ£€æµ‹è®­ç»ƒå®Œæˆ - æ£€æµ‹åˆ° {anomaly_count}/{len(features)} ä¸ªå¼‚å¸¸æ ·æœ¬")
            
            self.is_trained = True
            
            # åˆ†æå¼‚å¸¸æ¨¡å¼
            self._analyze_anomaly_patterns(features_scaled, anomaly_labels, user_ids, anomaly_scores)
            
            return True
            
        except Exception as e:
            logging.error(f"å¼‚å¸¸æ£€æµ‹è®­ç»ƒå¤±è´¥: {str(e)}")
            return False
    
    def _handle_outliers(self, features):
        """å¤„ç†å¼‚å¸¸å€¼"""
        features_clean = features.copy()
        
        for i in range(features.shape[1]):
            col = features[:, i]
            Q1 = np.percentile(col, 25)
            Q3 = np.percentile(col, 75)
            IQR = Q3 - Q1
            
            if IQR > 0:
                lower_bound = Q1 - 2.0 * IQR  # æ”¾å®½å¼‚å¸¸å€¼èŒƒå›´
                upper_bound = Q3 + 2.0 * IQR
                
                # ä¸æˆªæ–­ï¼Œä¿ç•™å¼‚å¸¸å€¼ä¿¡æ¯
                pass
        
        return features_clean
    
    def _analyze_anomaly_patterns(self, features, labels, user_ids, scores):
        """åˆ†æå¼‚å¸¸æ¨¡å¼"""
        self.anomaly_analysis = {
            'total_users': len(user_ids),
            'anomaly_count': sum(1 for label in labels if label == -1),
            'normal_count': sum(1 for label in labels if label == 1),
            'anomaly_rate': sum(1 for label in labels if label == -1) / len(labels) * 100,
            'anomalies': []
        }
        
        for i, (user_id, label, score) in enumerate(zip(user_ids, labels, scores)):
            if label == -1:  # å¼‚å¸¸æ ·æœ¬
                anomaly_info = {
                    'user_id': user_id,
                    'anomaly_score': float(score),
                    'severity': self._get_anomaly_severity(score),
                    'anomaly_types': self._identify_anomaly_types(features[i]),
                    'features': features[i].tolist()
                }
                self.anomaly_analysis['anomalies'].append(anomaly_info)
        
        # æŒ‰å¼‚å¸¸ç¨‹åº¦æ’åº
        self.anomaly_analysis['anomalies'].sort(key=lambda x: x['anomaly_score'])
    
    def _get_anomaly_severity(self, score):
        """ç¡®å®šå¼‚å¸¸ä¸¥é‡ç¨‹åº¦"""
        if score < -0.5:
            return 'high'
        elif score < -0.2:
            return 'medium'
        else:
            return 'low'
    
    def _identify_anomaly_types(self, features):
        """è¯†åˆ«å¼‚å¸¸ç±»å‹"""
        anomaly_types = []
        
        # ç‰¹å¾ç´¢å¼•å¯¹åº”
        homework_avg = features[0]
        homework_completion_rate = features[1]
        homework_consistency = features[2]
        discussion_posts = features[3]
        discussion_replies = features[4]
        upvotes_ratio = features[5]
        video_watch_time = features[6]
        video_rumination_ratio = features[7]
        learning_pattern_score = features[8]
        academic_performance = features[9]
        engagement_score = features[10]
        
        # åˆ¤æ–­å¼‚å¸¸ç±»å‹
        if engagement_score < -1:  # æ ‡å‡†åŒ–åå°äº-1è¡¨ç¤ºæä½
            anomaly_types.append('low_engagement')
        
        if homework_consistency < -1:
            anomaly_types.append('irregular_pattern')
        
        if academic_performance < -1.5:
            anomaly_types.append('poor_performance')
        
        if video_rumination_ratio > 1.5:  # è¿‡åº¦é‡å¤è§‚çœ‹
            anomaly_types.append('excessive_struggle')
        
        if (homework_completion_rate > 0.5 and academic_performance < -1) or \
           (discussion_posts + discussion_replies > 0 and academic_performance < -1):
            anomaly_types.append('inconsistent_behavior')
        
        return anomaly_types if anomaly_types else ['unknown']
    
    def detect_anomalies(self, user):
        """æ£€æµ‹å•ä¸ªç”¨æˆ·çš„å¼‚å¸¸è¡Œä¸º"""
        if not self.is_trained:
            return None
            
        try:
            features, _ = self.prepare_features([user])
            if len(features) == 0:
                return None
                
            features_scaled = self.scaler.transform(features)
            
            # é¢„æµ‹å¼‚å¸¸
            anomaly_label = self.model.predict(features_scaled)[0]
            anomaly_score = self.model.decision_function(features_scaled)[0]
            
            is_anomaly = anomaly_label == -1
            
            result = {
                'user_id': user.id,
                'is_anomaly': is_anomaly,
                'anomaly_score': float(anomaly_score),
                'severity': self._get_anomaly_severity(anomaly_score) if is_anomaly else 'normal',
                'confidence': 'high' if abs(anomaly_score) > 0.3 else 'medium',
                'recommendations': []
            }
            
            if is_anomaly:
                result['anomaly_types'] = self._identify_anomaly_types(features_scaled[0])
                result['recommendations'] = self._generate_anomaly_recommendations(result['anomaly_types'])
                result['alert_level'] = self._get_alert_level(anomaly_score, result['anomaly_types'])
            
            return result
            
        except Exception as e:
            logging.error(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")
            return None
    
    def _generate_anomaly_recommendations(self, anomaly_types):
        """æ ¹æ®å¼‚å¸¸ç±»å‹ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        type_recommendations = {
            'low_engagement': [
                "å»ºè®®è€å¸ˆä¸»åŠ¨è”ç³»è¯¥å­¦ç”Ÿ",
                "å®‰æ’å­¦ä¹ ä¼™ä¼´ååŠ©",
                "æä¾›é¢å¤–çš„å­¦ä¹ æ¿€åŠ±æªæ–½"
            ],
            'irregular_pattern': [
                "å»ºè®®åˆ¶å®šå›ºå®šçš„å­¦ä¹ æ—¶é—´è¡¨",
                "æä¾›æ—¶é—´ç®¡ç†æŒ‡å¯¼",
                "ç›‘ç£å­¦ä¹ è®¡åˆ’æ‰§è¡Œ"
            ],
            'poor_performance': [
                "å®‰æ’è¡¥å……è¾…å¯¼è¯¾ç¨‹",
                "é‡æ–°è¯„ä¼°å­¦ä¹ åŸºç¡€",
                "æä¾›ä¸ªæ€§åŒ–å­¦ä¹ æ–¹æ¡ˆ"
            ],
            'excessive_struggle': [
                "æ£€æŸ¥å­¦ä¹ æ–¹æ³•æ˜¯å¦å¾—å½“",
                "æä¾›æ›´å¤šåŸºç¡€èµ„æ–™",
                "å®‰æ’ä¸€å¯¹ä¸€ç­”ç–‘"
            ],
            'inconsistent_behavior': [
                "æ·±å…¥äº†è§£å­¦ç”Ÿå­¦ä¹ å›°éš¾",
                "è¯„ä¼°æ˜¯å¦å­˜åœ¨å¤–éƒ¨å› ç´ å½±å“",
                "æä¾›å¿ƒç†æ”¯æŒå’ŒæŒ‡å¯¼"
            ]
        }
        
        for anomaly_type in anomaly_types:
            if anomaly_type in type_recommendations:
                recommendations.extend(type_recommendations[anomaly_type])
        
        return list(set(recommendations))[:5]  # å»é‡å¹¶é™åˆ¶æ•°é‡
    
    def _get_alert_level(self, score, anomaly_types):
        """ç¡®å®šè­¦æŠ¥çº§åˆ«"""
        if score < -0.5 or 'poor_performance' in anomaly_types:
            return 'urgent'
        elif score < -0.2 or 'low_engagement' in anomaly_types:
            return 'warning'
        else:
            return 'attention'
    
    def batch_detect_anomalies(self, users):
        """æ‰¹é‡æ£€æµ‹å¼‚å¸¸è¡Œä¸º"""
        if not self.is_trained:
            return None
            
        try:
            features, user_ids = self.prepare_features(users)
            features_scaled = self.scaler.transform(features)
            
            # æ‰¹é‡é¢„æµ‹
            anomaly_labels = self.model.predict(features_scaled)
            anomaly_scores = self.model.decision_function(features_scaled)
            
            # ç»Ÿè®¡ç»“æœ
            anomalies = []
            for i, (user_id, label, score) in enumerate(zip(user_ids, anomaly_labels, anomaly_scores)):
                if label == -1:
                    anomaly_info = {
                        'user_id': user_id,
                        'anomaly_score': float(score),
                        'severity': self._get_anomaly_severity(score),
                        'anomaly_types': self._identify_anomaly_types(features_scaled[i])
                    }
                    anomalies.append(anomaly_info)
            
            # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
            anomalies.sort(key=lambda x: x['anomaly_score'])
            
            return {
                'total_users': len(user_ids),
                'anomaly_count': len(anomalies),
                'normal_count': len(user_ids) - len(anomalies),
                'anomaly_rate': len(anomalies) / len(user_ids) * 100 if len(user_ids) > 0 else 0,
                'anomalies': anomalies,
                'summary': self._generate_summary_report(anomalies)
            }
            
        except Exception as e:
            logging.error(f"æ‰¹é‡å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")
            return None
    
    def _generate_summary_report(self, anomalies):
        """ç”Ÿæˆå¼‚å¸¸æ£€æµ‹æ‘˜è¦æŠ¥å‘Š"""
        if not anomalies:
            return "æœªæ£€æµ‹åˆ°å¼‚å¸¸è¡Œä¸º"
        
        # ç»Ÿè®¡å¼‚å¸¸ç±»å‹åˆ†å¸ƒ
        type_counts = {}
        severity_counts = {'high': 0, 'medium': 0, 'low': 0}
        
        for anomaly in anomalies:
            # ç»Ÿè®¡ä¸¥é‡ç¨‹åº¦
            severity_counts[anomaly['severity']] += 1
            
            # ç»Ÿè®¡å¼‚å¸¸ç±»å‹
            for anomaly_type in anomaly['anomaly_types']:
                type_counts[anomaly_type] = type_counts.get(anomaly_type, 0) + 1
        
        # ç”ŸæˆæŠ¥å‘Š
        report_lines = []
        
        if severity_counts['high'] > 0:
            report_lines.append(f"ğŸš¨ å‘ç° {severity_counts['high']} ä¸ªé«˜é£é™©å­¦ç”Ÿ")
        
        if severity_counts['medium'] > 0:
            report_lines.append(f"âš ï¸ å‘ç° {severity_counts['medium']} ä¸ªä¸­ç­‰é£é™©å­¦ç”Ÿ")
        
        # ä¸»è¦å¼‚å¸¸ç±»å‹
        if type_counts:
            most_common_type = max(type_counts.items(), key=lambda x: x[1])
            type_name = self.anomaly_types.get(most_common_type[0], most_common_type[0])
            report_lines.append(f"ä¸»è¦é—®é¢˜ï¼š{type_name} ({most_common_type[1]}äºº)")
        
        return " | ".join(report_lines)
    
    def save_model(self, filepath):
        """ä¿å­˜å¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        if self.is_trained:
            model_data = {
                'model': self.model,
                'scaler': self.scaler,
                'contamination': self.contamination,
                'feature_names': self.feature_names,
                'anomaly_types': self.anomaly_types,
                'anomaly_analysis': getattr(self, 'anomaly_analysis', {})
            }
            joblib.dump(model_data, filepath)
            return True
        return False
    
    def load_model(self, filepath):
        """åŠ è½½å¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        try:
            model_data = joblib.load(filepath)
            self.model = model_data['model']
            self.scaler = model_data['scaler']
            self.contamination = model_data['contamination']
            self.feature_names = model_data['feature_names']
            self.anomaly_types = model_data['anomaly_types']
            self.anomaly_analysis = model_data.get('anomaly_analysis', {})
            self.is_trained = True
            return True
        except Exception as e:
            logging.error(f"å¼‚å¸¸æ£€æµ‹æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False